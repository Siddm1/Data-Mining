{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRvH6wRQeFpE"
      },
      "source": [
        "#Beta\n",
        "\n",
        "#####Siddhesh Mahadeshwar\n",
        "\n",
        "#####Zachary Noel\n",
        "\n",
        "#####Erin Dolson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfO5PuPjekim"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import graphviz\n",
        "%matplotlib inline\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn import datasets # import standard datasets\n",
        "from sklearn import tree     # decision tree classifier\n",
        "from sklearn import naive_bayes # naive bayes classifier \n",
        "from sklearn import svm        # svm classifier\n",
        "from sklearn.svm import SVC \n",
        "from sklearn import ensemble   # ensemble classifiers\n",
        "from sklearn import metrics    # performance evaluation metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing \n",
        "from sklearn import neighbors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6M95VPGhbsg"
      },
      "source": [
        "a) Load in the hit-movies data.  You will not use the original title and imdb id variables for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVHUroVngHF4"
      },
      "source": [
        "movies = pd.read_csv(\"hit-movies.csv\",\n",
        "                   engine='python')\n",
        "movies = movies[movies.columns.difference(['original_title', 'imdb_id'])]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H00jTM84hY-_"
      },
      "source": [
        "b) Prepare the data for a 10-fold cross-validation design, using the do-it-yourself approach Ensure that each split of the data has a balanced distribution of class labels. Use min-max scaling to ensure all variables fall between values of [0,1].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpd4VlBlgcH9"
      },
      "source": [
        "# Feature Matrix and Target Array\n",
        "X = movies[movies.columns.difference(['Hit'])]\n",
        "Y = movies['Hit']\n",
        "\n",
        "nFolds = 10\n",
        "kf = model_selection.StratifiedKFold(n_splits=nFolds, shuffle=True,\n",
        "random_state=3)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMyOJl5Uisbc"
      },
      "source": [
        "c) Use  kNN  to  predict  whether  a  movie  is  a  hit.   Estimate  the  generalization performance  over  the  folds,  report  the  mean  accuracy,  F1-measure,  and  AUC  on  the testing data fork values of 3, 9, and 15.\n",
        "\n",
        "d) Use decision trees to predict whether a movie is a hit.  Estimate the generalization performance over the folds, report the mean accuracy, F1-measure, and AUC on the testing data.  Show the results for two different sized trees (consider different amounts of pruning).\n",
        "\n",
        "e) Use a Naive Bayes classifier to predict whether a movie is a hit.  Report the mean accuracy, F1-measure, and AUC on the testing data over the folds.\n",
        "\n",
        "**All three parts are combined in 1 loop below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CJIT-vvRD1n"
      },
      "source": [
        "col_names = ['Knn3', 'Knn9', 'Knn15', 'Decision_Tree1', 'Decision_Tree2', 'NB']\n",
        "\n",
        "df = pd.DataFrame(columns=['Accuracy','F1-Measure','AUC'], \n",
        "index=['Knn3','Knn9', 'Knn15', 'Decision_Tree1', 'Decision_Tree2', 'NB', \n",
        "       'best SVM', 'best RF', 'best AdaBoost'])\n",
        "\n",
        "k_values=[3,9,15]\n",
        "sizes = [10, 100]\n",
        "\n",
        "for x in range(0,len(col_names)):\n",
        "  knn_kcv_scores = []\n",
        "  F1_measure_scores = []\n",
        "  AUC_scores = []\n",
        "\n",
        "  for tr_indx, te_indx in kf.split(X, Y):\n",
        "      # scale the data\n",
        "      x_train, x_test = X.iloc[tr_indx], X.iloc[te_indx]\n",
        "      y_train, y_test = Y[tr_indx], Y[te_indx]\n",
        "\n",
        "      y = MinMaxScaler()\n",
        "      x_train_transformed = y.fit_transform(x_train)\n",
        "      x_test_transformed = y.fit_transform(x_test)\n",
        "\n",
        "      # Q2c - Knn\n",
        "      if x < 3:\n",
        "        knn = neighbors.KNeighborsClassifier(n_neighbors=k_values[x])\n",
        "        knn.fit(x_train_transformed, y_train)\n",
        "        y_pred_te = knn.predict(x_test_transformed)\n",
        "      # Q2e - Decision Tree\n",
        "      elif x == 3 or x == 4:\n",
        "        clf = DecisionTreeClassifier(random_state=0, max_depth=sizes[x-4], class_weight={0:1.0, 1:6})\n",
        "        clf.fit(x_train_transformed, y_train)\n",
        "        y_pred_te = clf.predict(x_test_transformed)\n",
        "      #Q2d -NB\n",
        "      else:\n",
        "        gnb = naive_bayes.GaussianNB()\n",
        "        y_pred_test = gnb.fit(x_train_transformed, y_train)\n",
        "        y_pred_te = gnb.predict(x_test_transformed)\n",
        "     \n",
        "      # calculate scores or values  \n",
        "      knn_kcv_scores.append(metrics.accuracy_score(y_test, y_pred_te))\n",
        "      auc_score1 = roc_auc_score(y_test, y_pred_te)\n",
        "      f1_val = metrics.f1_score(y_test, y_pred_te, average='binary')\n",
        "      F1_measure_scores.append(f1_val)\n",
        "      AUC_scores.append(auc_score1)\n",
        "\n",
        "  knn_value = np.mean(knn_kcv_scores)\n",
        "  f1_value = np.mean(F1_measure_scores)\n",
        "  auc_value = np.mean(AUC_scores)\n",
        "\n",
        "  df.loc[col_names[x]] = pd.Series({'Accuracy':knn_value, 'F1-Measure':f1_value,\n",
        "                                    'AUC':auc_value})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upG0G8XyFeDp"
      },
      "source": [
        "f) Perform a second layer of cross-validation (k=5), an inner loop, to estimate the parameters of the following classifiers.  The inner loop of the cross-validation can make use of the methods of grid search to select the best parameterization of the following classifiers.  Or, you may elect to use the do-it-yourself approach with a nested loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdW5wL2aKPtG"
      },
      "source": [
        "i) Learn support vector machine (SVM) models to predict whether a movie is a hit.  You will consider multiple classifiers using both the RBF kernel (with default values) and polynomial kernel with degree 2, 3, and 4.  Consider values for cost penalty parameter of {0.01,0.1,1}.  Report the best parameter values (kernel + cost) for each outer fold (selected by AUC).\n",
        "\n",
        "ii) Use Random Forests to predict whether a movie is a hit. Consider multiple random  forests  with  the  number  of  trees  in  the  forest  to  be {25,50,100} and  the maximum  number  of  features  to  be {6,10,14}.   Report  the  best  parameter  values (number of trees + features) for each outer fold (selected by AUC).\n",
        "\n",
        "iii) Use  AdaBoost  to  predict  whether  a  movie  is  a  hit.   Consider  boosting methods with the number of decision stumps of{25,50}.  Report the best parameter values (number of stumps) for each outer fold (selected by AUC).\n",
        "\n",
        "**All 3 parts have been combined into a single loop below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE1M2y5lFdgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbca6f3d-04ec-477f-b9f8-71e7fb1127aa"
      },
      "source": [
        "gridsearch = ['best SVM', 'best RF', 'best AdaBoost']\n",
        "\n",
        "for num in range(0,len(gridsearch)):\n",
        "  print(str(gridsearch[num]))\n",
        "  # configure the cross-validation procedure\n",
        "  cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)\n",
        "\n",
        "  outer_results = list()\n",
        "  i = 0\n",
        "  for train_ix, test_ix in cv_outer.split(X):\n",
        "    accuracy_scores = []\n",
        "    F1_measure_scores = []\n",
        "    AUC_scores = []\n",
        "    # split data\n",
        "    X_train = X.iloc[train_ix, :]\n",
        "    X_test = X.iloc[test_ix, :]\n",
        "    y_train = Y[train_ix]\n",
        "    y_test = Y[test_ix]\n",
        "\n",
        "    # scale the data\n",
        "    y = MinMaxScaler().fit(X_train)\n",
        "    x_train_transformed = y.transform(X_train)\n",
        "    x_test_transformed = y.transform(X_test)\n",
        "    \n",
        "    # set up the cross-validation procedure\n",
        "    cv_inner = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "    # Q2f i - SVM\n",
        "    if num == 0:\n",
        "      # define the model\n",
        "      svmModel = SVC(random_state=1)\n",
        "      \n",
        "      # defining parameter range \n",
        "      param_grid = {'C': [0.01, 0.1, 1],  \n",
        "                    'kernel': ('rbf', 'poly'),\n",
        "                    'degree': [2, 3, 4]\n",
        "                    }  \n",
        "\n",
        "      svmSearch = GridSearchCV(svmModel, param_grid, scoring='roc_auc', \n",
        "                              cv=cv_inner, refit=True, n_jobs=5)\n",
        "      svmResult = svmSearch.fit(x_train_transformed, y_train)\n",
        "\n",
        "      # evaluate performance of best model\n",
        "      best_model_svm = svmResult.best_estimator_\n",
        "      yhat = best_model_svm.predict(x_test_transformed)\n",
        "      acc = accuracy_score(y_test, yhat)\n",
        "      auc = roc_auc_score(y_test, yhat)\n",
        "\n",
        "      # store the results\n",
        "      outer_results.append(auc)\n",
        "      \n",
        "      # report progress\n",
        "      print(' split=%d, auc=%.3f, est=%.3f, cfg=%s' % \n",
        "            (i, auc, svmResult.best_score_, svmResult.best_params_))\n",
        "      i += 1\n",
        "    # Q2f ii - Random Forest\n",
        "    elif num == 1:\n",
        "      # define the model\n",
        "      rfModel = RandomForestClassifier(random_state=1)\n",
        "      \n",
        "      # define search space, complete the search over the inner cv loop\n",
        "      rfSpace = dict()\n",
        "      rfSpace['n_estimators'] = [25, 50, 100]\n",
        "      rfSpace['max_features'] = [6, 10, 14]\n",
        "\n",
        "      rfSearch = GridSearchCV(rfModel, rfSpace, scoring='roc_auc',\n",
        "                              cv=cv_inner, \n",
        "                              refit=True)\n",
        "      rfResult = rfSearch.fit(x_train_transformed, y_train)\n",
        "\n",
        "      # evaluate performance of best model\n",
        "      best_model_rf = rfResult.best_estimator_\n",
        "      yhat = best_model_rf.predict(x_test_transformed)\n",
        "      acc = accuracy_score(y_test, yhat)\n",
        "      auc = roc_auc_score(y_test, yhat)\n",
        "\n",
        "      # store the results\n",
        "      outer_results.append(auc)\n",
        "      \n",
        "      # report progress\n",
        "      print(' split=%d, acc=%.3f, auc=%.3f, est=%.3f, cfg=%s' % \n",
        "(i, acc, auc, rfResult.best_score_, rfResult.best_params_))\n",
        "      i += 1\n",
        "    # Q2f iii - Adaboost\n",
        "    elif num == 2:\n",
        "      adaModel = ensemble.AdaBoostClassifier(random_state=1)\n",
        "\n",
        "      # define search space, complete the search over the inner cv loop\n",
        "      adaSpace = dict()\n",
        "      adaSpace['n_estimators'] = [25, 50]\n",
        "\n",
        "      adaSearch = GridSearchCV(adaModel, adaSpace, scoring='roc_auc',\n",
        "                                cv=cv_inner,\n",
        "                                refit=True)\n",
        "      adaResult = adaSearch.fit(x_train_transformed, y_train)\n",
        "\n",
        "      # evaluate performance of best model\n",
        "      best_model_ada = adaResult.best_estimator_\n",
        "      yhat = best_model_ada.predict(x_test_transformed)\n",
        "      acc = accuracy_score(y_test, yhat)\n",
        "      auc = roc_auc_score(y_test, yhat)\n",
        "\n",
        "      # store the results\n",
        "      outer_results.append(auc)\n",
        "      \n",
        "      # report progress\n",
        "      print(' split=%d, acc=%.3f, auc=%.3f, est=%.3f, cfg=%s' % \n",
        "(i, acc, auc, adaResult.best_score_, adaResult.best_params_))\n",
        "      i += 1\n",
        "        \n",
        "    # calculate accuracy\n",
        "    accuracy_scores.append(metrics.accuracy_score(y_test, yhat))\n",
        "\n",
        "    # calculate f1-score\n",
        "    F1_measure_scores.append(metrics.f1_score(y_test, yhat, average='binary'))\n",
        "\n",
        "    # calculate auc\n",
        "    AUC_scores.append(roc_auc_score(y_test, yhat))\n",
        "    \n",
        "# summarize the estimated performance of the model\n",
        "  accuracy_value = np.mean(accuracy_scores)\n",
        "  f1_value = np.mean(F1_measure_scores)\n",
        "  auc_value = np.mean(AUC_scores)\n",
        "\n",
        "  df.loc[gridsearch[num]] = pd.Series({'Accuracy':accuracy_value, \n",
        "                                       'F1-Measure':f1_value, 'AUC':auc_value})"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best SVM\n",
            " split=0, auc=0.500, est=0.675, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=1, auc=0.500, est=0.686, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=2, auc=0.500, est=0.662, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=3, auc=0.500, est=0.682, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=4, auc=0.500, est=0.678, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=5, auc=0.504, est=0.681, cfg={'C': 1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=6, auc=0.500, est=0.676, cfg={'C': 0.01, 'degree': 2, 'kernel': 'poly'}\n",
            " split=7, auc=0.500, est=0.674, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=8, auc=0.500, est=0.674, cfg={'C': 0.1, 'degree': 2, 'kernel': 'poly'}\n",
            " split=9, auc=0.500, est=0.679, cfg={'C': 1, 'degree': 2, 'kernel': 'poly'}\n",
            "best RF\n",
            " split=0, acc=0.845, auc=0.509, est=0.694, cfg={'max_features': 6, 'n_estimators': 100}\n",
            " split=1, acc=0.861, auc=0.534, est=0.693, cfg={'max_features': 14, 'n_estimators': 100}\n",
            " split=2, acc=0.808, auc=0.514, est=0.694, cfg={'max_features': 6, 'n_estimators': 100}\n",
            " split=3, acc=0.835, auc=0.502, est=0.692, cfg={'max_features': 10, 'n_estimators': 100}\n",
            " split=4, acc=0.855, auc=0.532, est=0.687, cfg={'max_features': 10, 'n_estimators': 100}\n",
            " split=5, acc=0.849, auc=0.538, est=0.692, cfg={'max_features': 10, 'n_estimators': 100}\n",
            " split=6, acc=0.818, auc=0.504, est=0.688, cfg={'max_features': 6, 'n_estimators': 100}\n",
            " split=7, acc=0.820, auc=0.503, est=0.695, cfg={'max_features': 6, 'n_estimators': 100}\n",
            " split=8, acc=0.820, auc=0.510, est=0.692, cfg={'max_features': 14, 'n_estimators': 100}\n",
            " split=9, acc=0.809, auc=0.513, est=0.693, cfg={'max_features': 6, 'n_estimators': 100}\n",
            "best AdaBoost\n",
            " split=0, acc=0.841, auc=0.499, est=0.643, cfg={'n_estimators': 50}\n",
            " split=1, acc=0.857, auc=0.504, est=0.636, cfg={'n_estimators': 50}\n",
            " split=2, acc=0.803, auc=0.503, est=0.641, cfg={'n_estimators': 25}\n",
            " split=3, acc=0.841, auc=0.498, est=0.647, cfg={'n_estimators': 50}\n",
            " split=4, acc=0.847, auc=0.505, est=0.641, cfg={'n_estimators': 50}\n",
            " split=5, acc=0.847, auc=0.509, est=0.639, cfg={'n_estimators': 50}\n",
            " split=6, acc=0.820, auc=0.500, est=0.630, cfg={'n_estimators': 25}\n",
            " split=7, acc=0.826, auc=0.506, est=0.634, cfg={'n_estimators': 50}\n",
            " split=8, acc=0.811, auc=0.498, est=0.642, cfg={'n_estimators': 50}\n",
            " split=9, acc=0.801, auc=0.497, est=0.642, cfg={'n_estimators': 50}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsnbeh40HxLg",
        "outputId": "c0323f05-d66e-4d74-9f2c-b6a3a514ac9f"
      },
      "source": [
        "# Final data frame.\n",
        "print(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Accuracy F1-Measure       AUC\n",
            "Knn3            0.788861    0.23008  0.549186\n",
            "Knn9            0.825359   0.141568   0.53045\n",
            "Knn15           0.829685  0.0887325  0.518646\n",
            "Decision_Tree1  0.758447   0.239266  0.546271\n",
            "Decision_Tree2  0.676407   0.342969  0.605828\n",
            "NB              0.617468   0.319137  0.582635\n",
            "best SVM        0.805142          0       0.5\n",
            "best RF         0.809202  0.0536913  0.513049\n",
            "best AdaBoost   0.801083          0  0.497479\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}